{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Iris Training and Prediction with Sagemaker Scikit-learn\n",
    " 이 자습서에서는 미리 빌드된 Scikit-learn [Scikit-learn](https://scikit-learn.org/stable/) 컨테이너를 사용하여 Sagemaker에서 를 사용하는 방법을 보여 줍니다.Scikit-learn은 인기있는 파이썬 기계 학습 프레임 워크입니다.여기에는 분류, 회귀, 클러스터링, 차원 감소 및 데이터/기능 전처리를위한 다양한 알고리즘이 포함됩니다.  \n",
    "\n",
    "[sagemaker-python-sdk](https://github.com/aws/sagemaker-python-sdk) 모듈을 사용하면 기존 scikit-learn 코드를 쉽게 가져올 수 있습니다. 이 코드는 IRIS 데이터 세트에 대한 모델을 훈련하고 예측 집합을 생성하여 보여줍니다. Scikit-learn container에 대한 더 많은 정보는 이곳을 확인하십시오. [sagemaker-scikit-learn-containers](https://github.com/aws/sagemaker-scikit-learn-container) repository and the [sagemaker-python-sdk](https://github.com/aws/sagemaker-python-sdk)\n",
    "\n",
    "Scikit-learn 에 대한 더 자세한 정보는 다음을 확인하십시오: <http://scikit-learn.org/stable/>.\n",
    "\n",
    "### Table of contents\n",
    "* [Upload the data for training](#upload_data)\n",
    "* [Create a Scikit-learn script to train with](#create_sklearn_script)\n",
    "* [Create the SageMaker Scikit Estimator](#create_sklearn_estimator)\n",
    "* [Train the SKLearn Estimator on the Iris data](#train_sklearn)\n",
    "* [Using the trained model to make inference requests](#inferece)\n",
    " * [Deploy the model](#deploy)\n",
    " * [Choose some data and use it for a prediction](#prediction_request)\n",
    " * [Endpoint cleanup](#endpoint_cleanup)\n",
    "* [Batch Transform](#batch_transform)\n",
    " * [Prepare Input Data](#prepare_input_data)\n",
    " * [Run Transform Job](#run_transform_job)\n",
    " * [Check Output Data](#check_output_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note: this example requires SageMaker Python SDK v2.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -U sagemaker>=2.15"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, lets create our Sagemaker session and role, and create a S3 prefix to use for the notebook example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# S3 prefix\n",
    "prefix = \"yudong-data/Scikit-iris\"\n",
    "\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "\n",
    "# Get a SageMaker-compatible role used by this Notebook Instance.\n",
    "role = get_execution_role()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'arn:aws:iam::806174985048:role/service-role/AmazonSageMaker-ExecutionRole-20201218T151409'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "role"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload the data for training <a class=\"anchor\" id=\"upload_data\"></a>\n",
    "\n",
    "방대한 양의 데이터가 포함된 대규모 모델을 훈련할 때는 일반적으로 Amazon Athena, AWS Gluse 또는 Amazon EMR과 같은 빅 데이터 도구를 사용하여 S3에 데이터를 생성합니다. 이 예제에서는 Scikit-learn에 포함된 아이리스 데이터 [Iris dataset](https://en.wikipedia.org/wiki/Iris_flower_data_set)를 사용합니다. 데이터를 로드하고 로컬로 작성한 다음 데이터를 s3에 씁니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "from sklearn import datasets\n",
    "\n",
    "# Load Iris dataset, then join labels and features\n",
    "iris = datasets.load_iris()\n",
    "joined_iris = np.insert(iris.data, 0, iris.target, axis=1)\n",
    "\n",
    "# Create directory and write csv\n",
    "os.makedirs(\"./data\", exist_ok=True)\n",
    "np.savetxt(\"./data/iris.csv\", joined_iris, delimiter=\",\", fmt=\"%1.1f, %1.3f, %1.3f, %1.3f, %1.3f\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "데이터를 로컬에 저장하면 SageMaker Python SDK에서 제공하는 도구를 사용하여 데이터를 기본 버킷에 업로드할 수 있습니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "WORK_DIRECTORY = \"data\"\n",
    "\n",
    "train_input = sagemaker_session.upload_data(\n",
    "    WORK_DIRECTORY, key_prefix=\"{}/{}\".format(prefix, WORK_DIRECTORY)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a Scikit-learn script to train with <a class=\"anchor\" id=\"create_sklearn_script\"></a>\n",
    "SageMaker는 'Sklearn' Estimator 를 사용하여 Scikit-learn 훈련 스크립트를 실행할 수 있습니다. SageMaker 에서 실행될 때 여러 유용한 환경 변수를 사용하여 교육 환경의 속성에 액세스할 수 있습니다. 예를 들면,\n",
    "\n",
    "* `SM_MODEL_DIR`: 모델 객체를 쓸 디렉토리의 경로를 나타내는 문자열입니다. 이 폴더에 저장된 모든 객체는 교육 작업이 완료된 후 모델 호스팅을 위해 S3에 업로드됩니다.\n",
    "* `SM_OUTPUT_DIR`: output 아티팩트를 쓸 파일 시스템 경로를 나타내는 문자열입니다. output 아티팩트에는 checkpoints, 그래프 및 저장할 기타 파일 (모델 아티팩트 제외) 이 포함될 수 있습니다. 이러한 아티팩트는 모델 아티팩트와 동일한 S3 접두사로 압축되어 S3에 업로드됩니다.\n",
    "\n",
    "'sklearn' Estimator 의 'fit ()' 메소드를 호출 할 때 두 개의 입력 채널 인 'train '과 'test'가 사용되었다고 가정하면 다음과 같은 환경 변수가 설정됩니다. `SM_CHANNEL_[channel_name]`:\n",
    "\n",
    "* `SM_CHANNEL_TRAIN`: 'train' 채널의 데이터를 포함하는 디렉토리 경로를 나타내는 문자열\n",
    "* `SM_CHANNEL_TEST`: 'test' 채널\n",
    "\n",
    "일반적인 훈련 스크립트는 입력 채널에서 데이터를 로드하고, 하이퍼파라미터를 사용하여 훈련을 구성하고, 모델을 학습하고, 모델을 model_dir 에 저장하여 나중에 호스팅할 수 있습니다. 하이퍼 매개 변수는 인수로 스크립트에 전달되고`argparse.argumentParser` 인스턴스로 검색 할 수 있습니다. 예를 들어, 이 노트북에서 실행할 스크립트는 다음과 같습니다:\n",
    "\n",
    "```python\n",
    "from __future__ import print_function\n",
    "\n",
    "import argparse\n",
    "import joblib\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn import tree\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    parser = argparse.ArgumentParser()\n",
    "\n",
    "    # Hyperparameters are described here. In this simple example we are just including one hyperparameter.\n",
    "    parser.add_argument('--max_leaf_nodes', type=int, default=-1)\n",
    "\n",
    "    # Sagemaker specific arguments. Defaults are set in the environment variables.\n",
    "    parser.add_argument('--output-data-dir', type=str, default=os.environ['SM_OUTPUT_DATA_DIR'])\n",
    "    parser.add_argument('--model-dir', type=str, default=os.environ['SM_MODEL_DIR'])\n",
    "    parser.add_argument('--train', type=str, default=os.environ['SM_CHANNEL_TRAIN'])\n",
    "\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    # Take the set of files and read them all into a single pandas dataframe\n",
    "    input_files = [ os.path.join(args.train, file) for file in os.listdir(args.train) ]\n",
    "    if len(input_files) == 0:\n",
    "        raise ValueError(('There are no files in {}.\\n' +\n",
    "                          'This usually indicates that the channel ({}) was incorrectly specified,\\n' +\n",
    "                          'the data specification in S3 was incorrectly specified or the role specified\\n' +\n",
    "                          'does not have permission to access the data.').format(args.train, \"train\"))\n",
    "    raw_data = [ pd.read_csv(file, header=None, engine=\"python\") for file in input_files ]\n",
    "    train_data = pd.concat(raw_data)\n",
    "\n",
    "    # labels are in the first column\n",
    "    train_y = train_data.iloc[:, 0]\n",
    "    train_X = train_data.iloc[:, 1:]\n",
    "\n",
    "    # Here we support a single hyperparameter, 'max_leaf_nodes'. Note that you can add as many\n",
    "    # as your training my require in the ArgumentParser above.\n",
    "    max_leaf_nodes = args.max_leaf_nodes\n",
    "\n",
    "    # Now use scikit-learn's decision tree classifier to train the model.\n",
    "    clf = tree.DecisionTreeClassifier(max_leaf_nodes=max_leaf_nodes)\n",
    "    clf = clf.fit(train_X, train_y)\n",
    "\n",
    "    # Print the coefficients of the trained classifier, and save the coefficients\n",
    "    joblib.dump(clf, os.path.join(args.model_dir, \"model.joblib\"))\n",
    "\n",
    "\n",
    "def model_fn(model_dir):\n",
    "    \"\"\"Deserialized and return fitted model\n",
    "    \n",
    "    Note that this should have the same name as the serialized model in the main method\n",
    "    \"\"\"\n",
    "    clf = joblib.load(os.path.join(model_dir, \"model.joblib\"))\n",
    "    return clf\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing scikit_learn_iris.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile scikit_learn_iris.py\n",
    "\n",
    "from __future__ import print_function\n",
    "\n",
    "import argparse\n",
    "import joblib\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn import tree\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    parser = argparse.ArgumentParser()\n",
    "\n",
    "    # Hyperparameters are described here. In this simple example we are just including one hyperparameter.\n",
    "    parser.add_argument('--max_leaf_nodes', type=int, default=-1)\n",
    "\n",
    "    # Sagemaker specific arguments. Defaults are set in the environment variables.\n",
    "    parser.add_argument('--output-data-dir', type=str, default=os.environ['SM_OUTPUT_DATA_DIR'])\n",
    "    parser.add_argument('--model-dir', type=str, default=os.environ['SM_MODEL_DIR'])\n",
    "    parser.add_argument('--train', type=str, default=os.environ['SM_CHANNEL_TRAIN'])\n",
    "\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    # Take the set of files and read them all into a single pandas dataframe\n",
    "    input_files = [ os.path.join(args.train, file) for file in os.listdir(args.train) ]\n",
    "    if len(input_files) == 0:\n",
    "        raise ValueError(('There are no files in {}.\\n' +\n",
    "                          'This usually indicates that the channel ({}) was incorrectly specified,\\n' +\n",
    "                          'the data specification in S3 was incorrectly specified or the role specified\\n' +\n",
    "                          'does not have permission to access the data.').format(args.train, \"train\"))\n",
    "    raw_data = [ pd.read_csv(file, header=None, engine=\"python\") for file in input_files ]\n",
    "    train_data = pd.concat(raw_data)\n",
    "\n",
    "    # labels are in the first column\n",
    "    train_y = train_data.iloc[:, 0]\n",
    "    train_X = train_data.iloc[:, 1:]\n",
    "\n",
    "    # Here we support a single hyperparameter, 'max_leaf_nodes'. Note that you can add as many\n",
    "    # as your training my require in the ArgumentParser above.\n",
    "    max_leaf_nodes = args.max_leaf_nodes\n",
    "\n",
    "    # Now use scikit-learn's decision tree classifier to train the model.\n",
    "    clf = tree.DecisionTreeClassifier(max_leaf_nodes=max_leaf_nodes)\n",
    "    clf = clf.fit(train_X, train_y)\n",
    "\n",
    "    # Print the coefficients of the trained classifier, and save the coefficients\n",
    "    joblib.dump(clf, os.path.join(args.model_dir, \"model.joblib\"))\n",
    "\n",
    "\n",
    "def model_fn(model_dir):\n",
    "    \"\"\"Deserialized and return fitted model\n",
    "    \n",
    "    Note that this should have the same name as the serialized model in the main method\n",
    "    \"\"\"\n",
    "    clf = joblib.load(os.path.join(model_dir, \"model.joblib\"))\n",
    "    return clf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scikit-Learn 컨테이너는 훈련 스크립트를 가져 오기 때문에 컨테이너가 실수로 훈련 코드를 실행하지 않도록 항상 주 가드 '__name__ == '__main__'' 에 훈련 코드를 넣어야합니다.\n",
    "\n",
    "For more information about training environment variables, please visit https://github.com/aws/sagemaker-containers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create SageMaker Scikit Estimator <a class=\"anchor\" id=\"create_sklearn_estimator\"></a>\n",
    "\n",
    "SgeMaker에서 Scikit-Learn 훈련 스크립트를 실행하기 위해, 우리는 몇 가지 생성자 인수를 받아들이는 `sagemaker.sklearn.estimator.sklearn` Estimator 를 구성합니다:\n",
    "\n",
    "* __entry_point__: SageMaker는 학습 및 예측을 위해 실행되는 파이썬 스크립트 경로입니다.\n",
    "* __role__: Role ARN\n",
    "* __instance_type__ *(optional)*: 학습을 위한 SageMaker 인스턴스의 유형입니다. __Note__: Because Scikit-learn does not natively support GPU training, Sagemaker Scikit-learn does not currently support training on GPU instance types.\n",
    "* __sagemaker_session__ *(optional)*: The session used to train on Sagemaker.\n",
    "* __hyperparameters__ *(optional)*: A dictionary passed to the train function as hyperparameters.\n",
    "\n",
    "To see the code for the SKLearn Estimator, see here: https://github.com/aws/sagemaker-python-sdk/tree/master/src/sagemaker/sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.sklearn.estimator import SKLearn\n",
    "\n",
    "FRAMEWORK_VERSION = \"0.23-1\"\n",
    "script_path = \"scikit_learn_iris.py\"\n",
    "\n",
    "sklearn = SKLearn(\n",
    "    entry_point=script_path,\n",
    "    framework_version=FRAMEWORK_VERSION,\n",
    "    instance_type=\"ml.c4.xlarge\",\n",
    "    role=role,\n",
    "    sagemaker_session=sagemaker_session,\n",
    "    hyperparameters={\"max_leaf_nodes\": 30},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train SKLearn Estimator on Iris data <a class=\"anchor\" id=\"train_sklearn\"></a>\n",
    "교육은 매우 간단합니다. Estimator 에 fit 이라는 함수를 호출합니다! 그러면 SageMaker 훈련 작업이 시작되어 데이터를 다운로드하고, 제공된 스크립트 파일에서 scikit-learn 코드를 호출하고, 스크립트가 생성하는 모든 모델 아티팩트를 저장합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-27 06:46:13 Starting - Starting the training job...\n",
      "2021-05-27 06:46:28 Starting - Launching requested ML instancesProfilerReport-1622097972: InProgress\n",
      "......\n",
      "2021-05-27 06:47:34 Starting - Preparing the instances for training......\n",
      "2021-05-27 06:48:44 Downloading - Downloading input data...\n",
      "2021-05-27 06:49:13 Training - Downloading the training image...\n",
      "2021-05-27 06:49:40 Uploading - Uploading generated training model\n",
      "2021-05-27 06:49:40 Completed - Training job completed\n",
      "\u001b[34m2021-05-27 06:49:27,827 sagemaker-containers INFO     Imported framework sagemaker_sklearn_container.training\u001b[0m\n",
      "\u001b[34m2021-05-27 06:49:27,829 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2021-05-27 06:49:27,839 sagemaker_sklearn_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[34m2021-05-27 06:49:28,315 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2021-05-27 06:49:28,328 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2021-05-27 06:49:28,339 sagemaker-training-toolkit INFO     No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2021-05-27 06:49:28,350 sagemaker-training-toolkit INFO     Invoking user script\n",
      "\u001b[0m\n",
      "\u001b[34mTraining Env:\n",
      "\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {\n",
      "        \"train\": \"/opt/ml/input/data/train\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"framework_module\": \"sagemaker_sklearn_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"max_leaf_nodes\": 30\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"train\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"is_master\": true,\n",
      "    \"job_name\": \"sagemaker-scikit-learn-2021-05-27-06-46-12-930\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-ap-northeast-2-806174985048/sagemaker-scikit-learn-2021-05-27-06-46-12-930/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"scikit_learn_iris\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 4,\n",
      "    \"num_gpus\": 0,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"scikit_learn_iris.py\"\u001b[0m\n",
      "\u001b[34m}\n",
      "\u001b[0m\n",
      "\u001b[34mEnvironment variables:\n",
      "\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={\"max_leaf_nodes\":30}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=scikit_learn_iris.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[\"train\"]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=scikit_learn_iris\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_sklearn_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=4\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=0\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://sagemaker-ap-northeast-2-806174985048/sagemaker-scikit-learn-2021-05-27-06-46-12-930/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"train\":\"/opt/ml/input/data/train\"},\"current_host\":\"algo-1\",\"framework_module\":\"sagemaker_sklearn_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"max_leaf_nodes\":30},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"is_master\":true,\"job_name\":\"sagemaker-scikit-learn-2021-05-27-06-46-12-930\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-ap-northeast-2-806174985048/sagemaker-scikit-learn-2021-05-27-06-46-12-930/source/sourcedir.tar.gz\",\"module_name\":\"scikit_learn_iris\",\"network_interface_name\":\"eth0\",\"num_cpus\":4,\"num_gpus\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\"],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"scikit_learn_iris.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--max_leaf_nodes\",\"30\"]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TRAIN=/opt/ml/input/data/train\u001b[0m\n",
      "\u001b[34mSM_HP_MAX_LEAF_NODES=30\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/miniconda3/bin:/miniconda3/lib/python37.zip:/miniconda3/lib/python3.7:/miniconda3/lib/python3.7/lib-dynload:/miniconda3/lib/python3.7/site-packages\n",
      "\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\n",
      "\u001b[0m\n",
      "\u001b[34m/miniconda3/bin/python scikit_learn_iris.py --max_leaf_nodes 30\n",
      "\n",
      "\u001b[0m\n",
      "\u001b[34m2021-05-27 06:49:30,139 sagemaker-containers INFO     Reporting training SUCCESS\u001b[0m\n",
      "Training seconds: 56\n",
      "Billable seconds: 56\n"
     ]
    }
   ],
   "source": [
    "sklearn.fit({\"train\": train_input})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the trained model to make inference requests <a class=\"anchor\" id=\"inference\"></a>\n",
    "\n",
    "### Deploy the model <a class=\"anchor\" id=\"deploy\"></a>\n",
    "\n",
    "모델을 SageMaker 호스팅에 배포하려면 적합 모델에 대한 'deploy' 함수 호출이 필요합니다. 이 호출은 인스턴스 수와 인스턴스 유형을 사용합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------!"
     ]
    }
   ],
   "source": [
    "predictor = sklearn.deploy(initial_instance_count=1, instance_type=\"ml.m5.xlarge\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choose some data and use it for a prediction <a class=\"anchor\" id=\"prediction_request\"></a>\n",
    "\n",
    "몇 가지 예측을 수행하기 위해 교육에 사용한 일부 데이터를 추출하고 이에 대한 예측을 수행합니다. 이것은 물론 나쁜 통계적 관행이지만 메커니즘이 어떻게 작동하는지 확인하는 좋은 방법입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import pandas as pd\n",
    "\n",
    "shape = pd.read_csv(\"data/iris.csv\", header=None)\n",
    "\n",
    "a = [50 * i for i in range(3)]\n",
    "b = [40 + i for i in range(10)]\n",
    "indices = [i + j for i, j in itertools.product(a, b)]\n",
    "\n",
    "test_data = shape.iloc[indices[:-1]]\n",
    "test_X = test_data.iloc[:, 1:]\n",
    "test_y = test_data.iloc[:, 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "예측은 배포에서 돌아온 예측 변수와 예측을 수행하려는 데이터로 예측을 호출하는 것만큼 쉽습니다. Endpoint의 출력은 분류 예측의 숫자 표현을 반환합니다. 원래 데이터 집합에서 이들은 꽃 이름이지만 이 예에서는 레이블은 숫자입니다.우리는 우리가 파싱 한 원래 레이블과 비교할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 2. 2. 2. 2.\n",
      " 2. 2. 2. 2. 2.]\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 2. 2. 2. 2.\n",
      " 2. 2. 2. 2. 2.]\n"
     ]
    }
   ],
   "source": [
    "print(predictor.predict(test_X.values))\n",
    "print(test_y.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Endpoint cleanup <a class=\"anchor\" id=\"endpoint_cleanup\"></a>\n",
    "\n",
    "Endpoint 작업이 완료되면 정리할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.delete_endpoint()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch Transform <a class=\"anchor\" id=\"batch_transform\"></a>\n",
    "또한 SageMaker Batch transform 을 사용하여 S3 데이터에 대한 비동기 배치 추론을 위해 훈련된 모델을 사용할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a SKLearn Transformer from the trained SKLearn Estimator\n",
    "transformer = sklearn.transformer(instance_count=1, instance_type=\"ml.m5.xlarge\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare Input Data <a class=\"anchor\" id=\"prepare_input_data\"></a>\n",
    "학습 데이터에서 100 행의 무작위 샘플을 10 개 추출한 다음 레이블 (Y) 에서 피처 (X) 를 분할합니다. 그런 다음 입력 데이터를 S3의 지정된 위치에 업로드합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# Randomly sample the iris dataset 10 times, then split X and Y\n",
    "mkdir -p batch_data/XY batch_data/X batch_data/Y\n",
    "for i in {0..9}; do\n",
    "    cat data/iris.csv | shuf -n 100 > batch_data/XY/iris_sample_${i}.csv\n",
    "    cat batch_data/XY/iris_sample_${i}.csv | cut -d',' -f2- > batch_data/X/iris_sample_X_${i}.csv\n",
    "    cat batch_data/XY/iris_sample_${i}.csv | cut -d',' -f1 > batch_data/Y/iris_sample_Y_${i}.csv\n",
    "done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload input data from local filesystem to S3\n",
    "batch_input_s3 = sagemaker_session.upload_data(\"batch_data/X\", key_prefix=prefix + \"/batch_input\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Transform Job <a class=\"anchor\" id=\"run_transform_job\"></a>\n",
    "Transformer 를 사용하여 S3 입력 데이터에 대해 변환 작업을 실행합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start a transform job and wait for it to finish\n",
    "transformer.transform(batch_input_s3, content_type=\"text/csv\")\n",
    "print(\"Waiting for transform job: \" + transformer.latest_transform_job.job_name)\n",
    "transformer.wait()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check Output Data  <a class=\"anchor\" id=\"check_output_data\"></a>\n",
    "변환 작업이 완료되면 S3에서 출력 데이터를 다운로드합니다. 입력 데이터의 각 파일 “F”에 대해 각 입력 행에서 예측 된 레이블을 포함하는 해당 파일 “f.out”이 있습니다. 우리는 이전에 저장된 실제 레이블과 예측 된 레이블을 비교할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the output data from S3 to local filesystem\n",
    "batch_output = transformer.output_path\n",
    "!mkdir -p batch_data/output\n",
    "!aws s3 cp --recursive $batch_output/ batch_data/output/\n",
    "# Head to see what the batch output looks like\n",
    "!head batch_data/output/*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# For each sample file, compare the predicted labels from batch output to the true labels\n",
    "for i in {1..9}; do\n",
    "    diff -s batch_data/Y/iris_sample_Y_${i}.csv \\\n",
    "        <(cat batch_data/output/iris_sample_X_${i}.csv.out | sed 's/[[\"]//g' | sed 's/, \\|]/\\n/g') \\\n",
    "        | sed \"s/\\/dev\\/fd\\/63/batch_data\\/output\\/iris_sample_X_${i}.csv.out/\"\n",
    "done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "DERIVED FROM:https://github.com/aws/sagemaker-python-sdk/blob/master/src/sagemaker/sklearn/README.rst\n",
    "Preparing the Scikit-learn training script\n",
    "Your Scikit-learn training script must be a Python 2.7 or 3.5 compatible source file.\n",
    "The training script is very similar to a training script you might run outside of SageMaker, \n",
    "but you can access useful properties about the training environment through various environment variables, \n",
    "such as\n",
    "- SM_MODEL_DIR: \n",
    "        A string representing the path to the directory to write model artifacts to. \n",
    "        These artifacts are uploaded to S3 for model hosting.\n",
    "- SM_OUTPUT_DATA_DIR: \n",
    "        A string representing the filesystem path to write output artifacts to. \n",
    "        Output artifacts may include checkpoints, graphs, and other files to save, \n",
    "        not including model artifacts. These artifacts are compressed and uploaded \n",
    "        to S3 to the same S3 prefix as the model artifacts.\n",
    "        Supposing two input channels, 'train' and 'test', \n",
    "        were used in the call to the Scikit-learn estimator's fit() method, \n",
    "        the following will be set, following the format \"SM_CHANNEL_[channel_name]\":\n",
    "- SM_CHANNEL_TRAIN: \n",
    "        A string representing the path to the directory containing data in the 'train' channel\n",
    "- SM_CHANNEL_TEST: \n",
    "        Same as above, but for the 'test' channel.\n",
    "        A typical training script loads data from the input channels, \n",
    "        configures training with hyperparameters, trains a model, \n",
    "        and saves a model to model_dir so that it can be hosted later. \n",
    "        Hyperparameters are passed to your script as arguments and can \n",
    "        be retrieved with an argparse.ArgumentParser instance. \n",
    "        For example, a training script might start with the following:\n",
    "Because the SageMaker imports your training script, \n",
    "you should put your training code in a main guard (if __name__=='__main__':) \n",
    "if you are using the same script to host your model, \n",
    "so that SageMaker does not inadvertently run your training code at the wrong point in execution.\n",
    "For more on training environment variables, please visit https://github.com/aws/sagemaker-containers.\n",
    "'''\n",
    "\n",
    "\n",
    "import argparse\n",
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import sklearn\n",
    "\n",
    "from sklearn import linear_model\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import six\n",
    "from six import StringIO, BytesIO\n",
    "\n",
    "# from sagemaker.content_types import CONTENT_TYPE_JSON, CONTENT_TYPE_CSV, CONTENT_TYPE_NPY\n",
    "# Interesting fact: \n",
    "#   on SageMaker model training instance, py-sagemaker is not installed\n",
    "# import sagemaker \n",
    "\n",
    "# matplotlib is not available \n",
    "# from matplotlib import pyplot as plt\n",
    "\n",
    "from sklearn.externals import joblib\n",
    "import json \n",
    "\n",
    "from sagemaker_containers.beta.framework import (\n",
    "    content_types, encoders, env, modules, transformer, worker)\n",
    "\n",
    "\n",
    "fn = 'sample_data.json'\n",
    "\n",
    "MA_list = [50, 100, 200, 300, 400, 800, 1600]\n",
    "intput_col = [\"MA-{}\".format(ma_lag) for ma_lag in MA_list[:-1]]\n",
    "benchmark_col = 'MA-1600'\n",
    "\n",
    "\n",
    "'''\n",
    "The RealTimePredictor used by Scikit-learn in the SageMaker \n",
    "Python SDK serializes NumPy arrays to the NPY format by default, \n",
    "with Content-Type application/x-npy. The SageMaker Scikit-learn model server \n",
    "can deserialize NPY-formatted data (along with JSON and CSV data).\n",
    "'''\n",
    "def input_fn(request_body, request_content_type):\n",
    "    \"\"\"An input_fn that loads a pickled numpy array\"\"\"\n",
    "    # print(\"request_body=\",str(request_body))\n",
    "    # print(\"np.load(StringIO(request_body))=\",np.load(StringIO(request_body)))\n",
    "\n",
    "    if request_content_type == \"application/python-pickle\":\n",
    "        array = np.load(BytesIO((request_body)))\n",
    "        # print(\"array=\",array)\n",
    "        return array\n",
    "    elif request_content_type == 'application/json':\n",
    "        jsondata = json.load(StringIO(request_body))\n",
    "        normalized_data, benchmark_data = process_input_data(jsondata)\n",
    "        # print(\"normalized_data=\",normalized_data)\n",
    "        return normalized_data, benchmark_data\n",
    "    else:\n",
    "        # Handle other content-types here or raise an Exception\n",
    "        # if the content type is not supported.\n",
    "        raise ValueError(\"{} not supported by script!\".format(request_content_type))\n",
    "\n",
    "def output_fn(prediction, accept):\n",
    "    \"\"\"Format prediction output\n",
    "    The default accept/content-type between containers for serial inference is JSON.\n",
    "    We also want to set the ContentType or mimetype as the same value as accept so the next\n",
    "    container can read the response payload correctly.\n",
    "    \"\"\"\n",
    "    if accept == \"application/json\":\n",
    "        return worker.Response(json.dumps(prediction), accept, mimetype=accept)\n",
    "    elif accept == 'text/csv':\n",
    "        return worker.Response(encoders.encode(prediction, accept), accept, mimetype=accept)\n",
    "    else:\n",
    "        raise ValueError(\"{} accept type is not supported by this script.\".format(accept))\n",
    "\n",
    "def predict_fn(input_data, model):\n",
    "    \"\"\"Preprocess input data\n",
    "    We implement this because the default predict_fn uses .predict(), but our model is a preprocessor\n",
    "    so we want to use .transform().\n",
    "    The output is returned in the following order:\n",
    "        rest of features either one hot encoded or standardized\n",
    "    \"\"\"\n",
    "    normalized_data, benchmark_data = input_data\n",
    "    \n",
    "    prediction = model.predict(normalized_data)\n",
    "    \n",
    "    output = np.array(prediction) * np.array(benchmark_data)\n",
    "    \n",
    "    return {'prediction-base-time': str(normalized_data.index[-1]), \n",
    "            'predicted-value': output[-1]}\n",
    "\n",
    "def model_fn(model_dir):\n",
    "    clf = joblib.load(os.path.join(model_dir, \"model.joblib\"))\n",
    "    return clf\n",
    "\n",
    "def process_input_data(cmcjsondata, for_training = False):\n",
    "    raw_data = pd.DataFrame(cmcjsondata)\n",
    "    dat = pd.DataFrame(list(raw_data['price_usd']), columns=['timestamp', 'usd'])\n",
    "    dat['dt_utc'] = pd.to_datetime(dat['timestamp']*1e6)\n",
    "    dat = dat.set_index('dt_utc')\n",
    "    resampled_data = dat['usd'].resample('30S').mean().interpolate('linear')\n",
    "\n",
    "    feature = {}\n",
    "\n",
    "    if for_training:\n",
    "        Y = resampled_data.rolling(2880).median().shift(-2879) # next 24 hour\n",
    "        feature['Y'] = Y\n",
    "\n",
    "    for ma_lag in MA_list:\n",
    "        feature[\"MA-{}\".format(ma_lag)] = resampled_data.rolling(ma_lag).mean()\n",
    "\n",
    "    data = pd.DataFrame(feature).dropna()\n",
    "    benchmark_data = data[benchmark_col].copy()\n",
    "    normalized_data = data.div(data[benchmark_col], axis=0)\n",
    "    \n",
    "    _col = intput_col + (['Y'] if for_training else [])\n",
    "\n",
    "    return normalized_data[_col], benchmark_data\n",
    "    \n",
    "def run_training(args):\n",
    "\n",
    "    with open(os.path.join(args.train, fn)) as fp:\n",
    "        jsondata = json.load(fp)\n",
    "    normalized_data, _ = process_input_data(jsondata, for_training = True)\n",
    "\n",
    "    model = linear_model.Ridge()\n",
    "    model.fit(normalized_data[intput_col], normalized_data['Y'])\n",
    "\n",
    "    joblib.dump(model, os.path.join(args.model_dir, \"model.joblib\"))\n",
    "\n",
    "\n",
    "if __name__ =='__main__':\n",
    "\n",
    "    parser = argparse.ArgumentParser()\n",
    "\n",
    "    # hyperparameters sent by the client are passed as command-line arguments to the script.\n",
    "    parser.add_argument('--epochs', type=int, default=50)\n",
    "    parser.add_argument('--batch-size', type=int, default=64)\n",
    "    parser.add_argument('--learning-rate', type=float, default=0.05)\n",
    "\n",
    "    # Data, model, and output directories\n",
    "    parser.add_argument('--output-data-dir', type=str, default=os.environ.get('SM_OUTPUT_DATA_DIR'))\n",
    "    parser.add_argument('--model-dir', type=str, default=os.environ.get('SM_MODEL_DIR'))\n",
    "    parser.add_argument('--train', type=str, default=os.environ.get('SM_CHANNEL_TRAIN'))\n",
    "    parser.add_argument('--test', type=str, default=os.environ.get('SM_CHANNEL_TEST'))\n",
    "\n",
    "    args, _ = parser.parse_known_args()\n",
    "\n",
    "    run_training(args)\n",
    "\n",
    "    # ... load from args.train and args.test, train a model, write model to args.model_dir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
